{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Important things:\n",
    "    * Problem statement\n",
    "    * Data!\n",
    "    * Model\n",
    "    * Optimization\n",
    "    * Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given a grayscale 16x16 image we first split it into four non-overlapping 4x16 patches.\n",
    "We now want to decide which part has the highest average intensity.\n",
    "\n",
    "We can reshape each image to a 256-d vector.\n",
    "If v is such a vector, then:\n",
    "    * v[:64] corresponds to the first patch\n",
    "    * v[64:128] corresponds to the second patch\n",
    "    * v[128:192] corresponds to the third patch\n",
    "    * v[192:] corresponds to the fourth patch\n",
    "\n",
    "We'll treat this as a 4-way classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: PyTorch Dataset and DataLoader classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "        # Generate N random samples\n",
    "        self.samples = torch.rand(N, 256)\n",
    "        # Generate labels\n",
    "        _labels = self.samples.reshape(N, 4, -1).mean(-1)  # (N, 4)\n",
    "        self.labels = _labels.argmax(1)  # (N,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index], self.labels[index]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    MyDataset(40000),\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    MyDataset(10000),\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - 0.5\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def single_epoch_loop(model, loader, optimizer=None, mode='train'):\n",
    "    n_samples, n_correct = 0, 0\n",
    "    # Loop through my data loader\n",
    "    for batch in loader:\n",
    "        samples, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(samples)\n",
    "\n",
    "        # Loss: multi-class classification -> cross-entropy\n",
    "        loss = F.cross_entropy(out, labels)  # no explicit softmax on out!\n",
    "\n",
    "        # Backward pass\n",
    "        if mode == 'train':  # don't update if not train\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        n_samples += len(samples)\n",
    "        n_correct += (out.argmax(1) == labels).sum().item()\n",
    "    return n_correct / n_samples, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together: our train/test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6664162660256411 0.7797598838806152 0.9014 0.8465317487716675\n",
      "1 0.9112830528846154 0.3555120527744293 0.9583 0.37055617570877075\n",
      "2 0.9362479967948718 0.2592712938785553 0.9655 0.24488228559494019\n",
      "3 0.9442608173076923 0.19257815182209015 0.9695 0.2047320455312729\n",
      "4 0.9473157051282052 0.22139495611190796 0.9732 0.17845094203948975\n",
      "5 0.9496444310897436 0.16154563426971436 0.9726 0.14041639864444733\n",
      "6 0.9524238782051282 0.16834506392478943 0.9736 0.13506561517715454\n",
      "7 0.9537760416666666 0.1187991127371788 0.9741 0.14165055751800537\n",
      "8 0.9550530849358975 0.10179155319929123 0.9761 0.15123522281646729\n",
      "9 0.9585586939102564 0.13134028017520905 0.9741 0.1254160851240158\n",
      "10 0.9584084535256411 0.1346886157989502 0.9774 0.11818530410528183\n",
      "11 0.9592848557692307 0.1368156224489212 0.9767 0.11666432023048401\n",
      "12 0.9622145432692307 0.11158645153045654 0.975 0.09952695667743683\n",
      "13 0.962089342948718 0.10493985563516617 0.9743 0.08962585777044296\n",
      "14 0.9641426282051282 0.13586169481277466 0.9745 0.0938771516084671\n",
      "15 0.9643179086538461 0.11007781326770782 0.9751 0.13132552802562714\n",
      "16 0.9667718349358975 0.0691039115190506 0.9745 0.10672980546951294\n",
      "17 0.9671975160256411 0.07856656610965729 0.9763 0.09892234951257706\n",
      "18 0.9677734375 0.06828200817108154 0.9764 0.10270310938358307\n",
      "19 0.9686748798076923 0.14820215106010437 0.9755 0.09089408069849014\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "\n",
    "\n",
    "model = MyModel(256, 4)\n",
    "# optimizer = SGD(model.parameters(), lr=1e-1)\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_acc, train_loss = single_epoch_loop(model, train_loader, optimizer, mode='train')\n",
    "\n",
    "    model.eval()  # disables dropout/batchnorm update etc\n",
    "    with torch.no_grad():  # disable gradient computations\n",
    "        test_acc, test_loss = single_epoch_loop(model, test_loader, mode='test')\n",
    "    print(\n",
    "        epoch, train_acc, train_loss,\n",
    "        test_acc, test_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save({\n",
    "    \"epoch\": epoch + 1,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"best_acc\": test_acc\n",
    "}, 'ckpt.pt')\n",
    "\n",
    "# Load\n",
    "ckpnt = torch.load('ckpt.pt')\n",
    "model.load_state_dict(ckpnt[\"model_state_dict\"], strict=True)\n",
    "optimizer.load_state_dict(ckpnt[\"optimizer_state_dict\"])\n",
    "start_epoch = ckpnt[\"epoch\"]\n",
    "val_acc_prev_best = ckpnt['best_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do better in real-world projects?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* Monitor losses/metrics in TensorBoard/wandb.\n",
    "* Use learning rate schedulers.\n",
    "* For images, use torchvision for loading, augmentations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
